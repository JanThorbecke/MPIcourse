-1-
Implement a parallel matrix-vector multiplication Ax=y such that a root task has the initial full arrays (e.g. as from an input file), and after the multiplication, all tasks have the vector y. A simple parallelization strategy is to divide A into blocks of some number of rows and broadcast the whole x to all tasks. The scattering of the matrix will be different in C and Fortran (due to how the languages place the arrays in memory) - see mxv.f90 and mxv.c and insert the appropriate collective operations.

  MPI_Scatter(A, ****, MPI_FLOAT, Aloc, *****, MPI_FLOAT, 0,MPI_COMM_WORLD);
  MPI_Bcast(x, ****, MPI_FLOAT, root, MPI_COMM_WORLD);

  MPI_Allgather(yloc, ****, MPI_FLOAT, y, ****, MPI_FLOAT, MPI_COMM_WORLD);


-2-
Insert OpenMP directives in mxv_mpi.c/f90

check the performance between pure MPI and hybrid. 

Compile with 
mpicc/f90 -qopenmp (Intel compiler)
mpicc/f90 -fopenmp (GNU compiler)

run with:
export OMP_NUM_THREADS=8
mpirun -np 2 ./a.out 10000

